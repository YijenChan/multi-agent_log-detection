import pandas as pd
import os
import logging
from tqdm import tqdm
from collections import deque
from settings import parse_settings
from heapq import heappush, heappop

def setup_logging(output_dir, log_type, radio):
    """é…ç½®å¢å¼ºå‹æ—¥å¿—è®°å½•"""
    log_file = os.path.join(output_dir, f"{log_type}_opt_{radio}_sample.log")
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.handlers = []
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setFormatter(formatter)
    
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)


import heapq

def dynamic_allocation(distribution, target):
    """
    å®Œå…¨åˆ†é…ç›®æ ‡é‡‡æ ·æ•°çš„æ”¹è¿›ç®—æ³•
    Args:
        distribution: äº‹ä»¶åˆ†å¸ƒå­—å…¸ {event_id: count}
        target: ç›®æ ‡é‡‡æ ·æ€»æ•°
    
    Returns:
        allocation: å®Œå…¨åˆ†é…åçš„æ–¹æ¡ˆå­—å…¸
    """
    total_events = sum(distribution.values())
    num_types = len(distribution)
    
    # è¾“å…¥éªŒè¯
    if target > total_events:
        raise ValueError(f"ç›®æ ‡é‡‡æ ·æ•°{target}è¶…è¿‡æ€»äº‹ä»¶æ•°{total_events}")
    if target < num_types:
        raise ValueError(f"ç›®æ ‡é‡‡æ ·æ•°{target}å°äºäº‹ä»¶ç±»å‹æ•°{num_types}")

    # åˆå§‹åŒ–åˆ†é…ï¼ˆæ¯ä¸ªç±»å‹è‡³å°‘1ä¸ªï¼‰
    allocation = {e:1 for e in distribution}
    remaining = target - num_types
    
    # åˆ›å»ºæœ€å¤§å †ï¼ˆä½¿ç”¨è´Ÿæ•°å®ç°ï¼‰
    heap = []
    for event, count in distribution.items():
        available = count - 1  # å·²åˆ†é…1ä¸ª
        if available > 0:
            heapq.heappush(heap, (-available, event))

    # æ‰§è¡Œå‰©ä½™åˆ†é…
    while remaining > 0 and heap:
        neg_avail, event = heapq.heappop(heap)
        avail = -neg_avail
        
        # åˆ†é…1ä¸ªæ ·æœ¬
        allocation[event] += 1
        remaining -= 1
        avail -= 1
        
        # é‡æ–°å…¥å †ï¼ˆå¦‚æœè¿˜æœ‰å¯ç”¨é‡ï¼‰
        if avail > 0:
            heapq.heappush(heap, (-avail, event))

    if remaining > 0:
        raise RuntimeError(f"åˆ†é…æœªå®Œæˆï¼Œå‰©ä½™{remaining}ä¸ªæœªåˆ†é…")

    return allocation

def proportional_allocation(distribution, target):
    """
    æŒ‰æ¯”ä¾‹åˆ†é…ä¸”ä¿è¯å®Œå…¨åˆ†é…çš„ç®—æ³•
    Args:
        distribution: äº‹ä»¶åˆ†å¸ƒå­—å…¸ {event_id: count}
        target: ç›®æ ‡é‡‡æ ·æ€»æ•°
    
    Returns:
        allocation: åˆ†é…æ–¹æ¡ˆå­—å…¸
    """
    total_events = sum(distribution.values())
    num_types = len(distribution)
    
    # è¾“å…¥éªŒè¯
    if target > total_events:
        raise ValueError(f"ç›®æ ‡é‡‡æ ·æ•°{target}è¶…è¿‡æ€»äº‹ä»¶æ•°{total_events}")
    if target < num_types:
        raise ValueError(f"ç›®æ ‡é‡‡æ ·æ•°{target}å°äºäº‹ä»¶ç±»å‹æ•°{num_types}")

    # è®¡ç®—æ¯ä¸ªäº‹ä»¶çš„ç†è®ºåˆ†é…æ¯”ä¾‹
    proportions = {e: count/total_events for e, count in distribution.items()}
    
    # ç¬¬ä¸€é˜¶æ®µï¼šæ¯ä¸ªäº‹ä»¶è‡³å°‘åˆ†é…1ä¸ª
    allocation = {e:1 for e in distribution}
    remaining = target - num_types
    
    # ç¬¬äºŒé˜¶æ®µï¼šæŒ‰æ¯”ä¾‹åˆ†é…å‰©ä½™æ•°é‡
    additional_allocation = {}
    for e in distribution:
        expected = proportions[e] * remaining
        additional_allocation[e] = expected
    
    # å¤„ç†æ•´æ•°åˆ†é…
    allocated = 0
    sorted_events = sorted(additional_allocation.items(), 
                         key=lambda x: x[1] - int(x[1]), 
                         reverse=True)  # æŒ‰å°æ•°éƒ¨åˆ†æ’åº
    
    # å…ˆåˆ†é…æ•´æ•°éƒ¨åˆ†
    for e, amount in sorted_events:
        integer_part = int(amount)
        allocation[e] += integer_part
        allocated += integer_part
    
    # åˆ†é…ä½™æ•°ï¼ˆæŒ‰å°æ•°éƒ¨åˆ†ä»å¤§åˆ°å°ï¼‰
    remainder = remaining - allocated
    for e, amount in sorted_events[:remainder]:
        allocation[e] += 1
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šä¿®æ­£è¶…å‡ºå®é™…å¯ç”¨é‡çš„æƒ…å†µ
    heap = []
    for e in allocation:
        available = distribution[e]
        if allocation[e] > available:
            over = allocation[e] - available
            allocation[e] = available
            heapq.heappush(heap, (-over, e))  # æœ€å¤§å †å­˜å‚¨éœ€è¦è¡¥è¶³çš„æ•°é‡
    
    # é‡æ–°åˆ†é…æº¢å‡ºçš„é…é¢
    while heap:
        need, e = heapq.heappop(heap)
        need = -need
        
        # å¯»æ‰¾å¯ä»¥å¢åŠ é…é¢çš„äº‹ä»¶
        candidates = []
        for event in distribution:
            if allocation[event] < distribution[event]:
                candidates.append(event)
        
        if not candidates:
            raise RuntimeError("æ— æ³•å®Œæˆåˆ†é…ï¼šæ— å¯ç”¨å€™é€‰äº‹ä»¶")
        
        # æŒ‰æ¯”ä¾‹åˆ†é…éœ€è¦è¡¥è¶³çš„æ•°é‡
        candidate_weights = [distribution[e] - allocation[e] for e in candidates]
        total_weight = sum(candidate_weights)
        
        if total_weight == 0:
            raise RuntimeError("æ— æ³•å®Œæˆåˆ†é…ï¼šæ‰€æœ‰äº‹ä»¶é…é¢å·²ç”¨å°½")
        
        for event in candidates:
            can_add = min(
                need * (candidate_weights[candidates.index(event)] / total_weight),
                distribution[event] - allocation[event]
            )
            allocation[event] += int(can_add)
            need -= int(can_add)
            if need <= 0:
                break
    
    return allocation


def optimized_sampling(log_type, radio, scheme='anomaly_based'):
    """ä¼˜åŒ–åçš„é‡‡æ ·ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–é…ç½®
    config = parse_settings[log_type]
    headers = config['headers']
    line_id_col = next(h for h in headers if 'LineId' in h)
    event_id_col = next(h for h in headers if 'EventId' in h)
    
    # è·¯å¾„é…ç½®
    log_data_dir = './dataset'
    input_dir = os.path.join(log_data_dir, log_type)
    raw_log_file = os.path.join(input_dir, f"{log_type}.log_structured.csv")
    output_file = os.path.join(input_dir, f"{log_type}_opt_{radio}.csv")
    
    setup_logging(input_dir, log_type, radio)
    
    # é«˜æ•ˆæ•°æ®è¯»å–
    logging.info("ğŸ”„ å¼€å§‹è¯»å–æ•°æ®æ–‡ä»¶...")
    chunks = []
    for chunk in tqdm(pd.read_csv(raw_log_file, chunksize=100000, header=0, names=headers),
                      desc="æ•°æ®è¯»å–è¿›åº¦",
                      unit="chunk"):
        chunks.append(chunk)
    df = pd.concat(chunks, axis=0)
    
    # æ•°æ®é¢„å¤„ç†
    logging.info("ğŸ” è¿›è¡Œæ•°æ®é¢„å¤„ç†...")
    if scheme == 'anomaly_based':
        anomaly_condition = df[headers[1]].isin(config['anomaly_labels'])
    else:
        anomaly_condition = ~df[headers[1]].isin(config['normal_labels'])
    
    anomaly_df = df[anomaly_condition].copy()
    normal_df = df[~anomaly_condition].copy()
    total_normal = len(normal_df)
    total_anomaly = len(anomaly_df)
    
    # è®¡ç®—ç›®æ ‡é‡‡æ ·æ•°
    original_target_anomaly = total_normal // radio
    target_anomaly = original_target_anomaly
    allocation = None
    logging.info(f"ğŸ“Š æ ·æœ¬å¹³è¡¡å‚æ•°ï¼šæ­£å¸¸:å¼‚å¸¸ = {radio}:1")
    logging.info(f"ğŸ“ˆ ç›®æ ‡å¼‚å¸¸æ ·æœ¬æ•°ï¼š{target_anomaly}ï¼ˆåŸºäº{total_normal}æ­£å¸¸æ ·æœ¬ï¼‰")
     
    # å¼‚å¸¸äº‹ä»¶åˆ†å¸ƒåˆ†æ
    anomaly_dist = anomaly_df[event_id_col].value_counts().to_dict()
    logging.info(f"\nåŸå§‹å¼‚å¸¸äº‹ä»¶åˆ†å¸ƒï¼ˆå…±{len(anomaly_dist)}ç±»ï¼‰ï¼š")
    # for event, count in sorted(anomaly_dist.items(), key=lambda x: x[1], reverse=True):
    #     logging.info(f"  - {event}: {count} æ¡")
    logging.info(f"{anomaly_dist}")

    # å¼‚å¸¸æ ·æœ¬ä¸è¶³å¤„ç†é€»è¾‘
    if total_anomaly < original_target_anomaly:
        logging.warning(f"å¼‚å¸¸æ ·æœ¬ä¸è¶³ï¼šå®é™…{total_anomaly} < ç›®æ ‡{original_target_anomaly}")
        logging.info("å¯åŠ¨åº”æ€¥å¤„ç†æ–¹æ¡ˆï¼šä½¿ç”¨å…¨éƒ¨å¼‚å¸¸æ ·æœ¬ï¼Œè°ƒæ•´æ­£å¸¸æ ·æœ¬æ¯”ä¾‹")
        target_anomaly = total_anomaly
    else:
        # ç”Ÿæˆæ­£å¸¸åˆ†é…æ–¹æ¡ˆ
        try:
            # åŠ¨æ€åˆ†é…æ–¹æ¡ˆç”Ÿæˆ
            logging.info("\nç”ŸæˆåŠ¨æ€åˆ†é…æ–¹æ¡ˆ...")
            allocation = proportional_allocation(anomaly_dist, target_anomaly)
        except (ValueError, RuntimeError) as e:
            logging.error(f"åˆ†é…å¤±è´¥: {str(e)}")
            return


    # æ–¹æ¡ˆå±•ç¤ºå’Œç¡®è®¤
    if allocation is not None:
        actual_total = sum(allocation.values())
        logging.info(f"\nåˆ†é…æ–¹æ¡ˆæ¦‚è§ˆï¼ˆç›®æ ‡ï¼š{target_anomaly}ï¼Œå®é™…ï¼š{actual_total}ï¼‰:")

        if len(allocation) > 10:
            logging.info("åˆ†é…æ–¹æ¡ˆæ‘˜è¦ï¼š")
            logging.info(f"äº‹ä»¶ç±»å‹æ•°ï¼š{len(allocation)}")
            logging.info(f"æœ€å¤§å•äº‹ä»¶é‡‡æ ·æ•°ï¼š{max(allocation.values())}")
            logging.info(f"æœ€å°å•äº‹ä»¶é‡‡æ ·æ•°ï¼š{min(allocation.values())}")
            logging.info("å®Œæ•´åˆ†é…æ–¹æ¡ˆï¼š")
            logging.info(allocation)
        else:
            for event, count in sorted(allocation.items(), key=lambda x: x[1], reverse=True):
                original = anomaly_dist[event]
                ratio = count/original if original >0 else 0
                logging.info(f"{event}: {count}/{original} ({ratio:.1%})")
        
        user_input = input("\nç¡®è®¤æ‰§è¡Œé‡‡æ ·ï¼Ÿï¼ˆY/Nï¼‰ ").strip().upper()
        if user_input != 'Y':
            logging.warning("æ“ä½œå·²å–æ¶ˆ")
            return
    
    # æ‰§è¡Œå¼‚å¸¸æ ·æœ¬é‡‡æ ·
    logging.info("\næ‰§è¡Œå¼‚å¸¸æ ·æœ¬é‡‡æ ·...")
    if allocation:
        sampled_anomaly = []
        for event_id, quota in allocation.items():
            event_data = anomaly_df[anomaly_df[event_id_col] == event_id]
            sample_size = min(quota, len(event_data))
            sampled = event_data.sample(n=sample_size, random_state=42)
            sampled_anomaly.append(sampled)
            logging.info(f"{event_id}: é‡‡æ · {len(sampled)}/{quota} æ¡")
        anomaly_sample = pd.concat(sampled_anomaly)
    else:
        anomaly_sample = anomaly_df.copy()
        logging.info(f"ä½¿ç”¨å…¨éƒ¨å¼‚å¸¸æ ·æœ¬ï¼š{len(anomaly_sample)} æ¡")

    final_anomaly_count = len(anomaly_sample)
    
    # è®¡ç®—æ‰€éœ€æ­£å¸¸æ ·æœ¬
    required_normal = final_anomaly_count * radio
    logging.info(f"\næœ€ç»ˆéœ€é‡‡æ ·æ­£å¸¸æ ·æœ¬ï¼š{required_normal} æ¡")

    # æ­£å¸¸æ ·æœ¬å¤„ç†
    if len(normal_df) < required_normal:
        logging.error("æ­£å¸¸æ ·æœ¬ä¸è¶³ï¼Œæ— æ³•å®Œæˆé‡‡æ ·")
        raise ValueError(f"æ­£å¸¸æ ·æœ¬ä¸è¶³ï¼šéœ€è¦{required_normal} ç°æœ‰{len(normal_df)}")
    
    if len(normal_df) > required_normal:
        drop_count = len(normal_df) - required_normal
        logging.info(f"åˆ é™¤å¤šä½™æ­£å¸¸æ ·æœ¬ï¼š{drop_count} æ¡")
        normal_sample = normal_df.sample(n=required_normal, random_state=42)
    else:
        normal_sample = normal_df.copy()
        logging.info("ä½¿ç”¨å…¨éƒ¨æ­£å¸¸æ ·æœ¬")
    
    # ç»“æœæ•´åˆ
    logging.info("\nğŸ”— åˆå¹¶é‡‡æ ·ç»“æœ...")
    final_df = pd.concat([anomaly_sample, normal_sample])
    
    # æ•°æ®åå¤„ç†
    logging.info("ğŸ“ ç”Ÿæˆæœ€ç»ˆæ•°æ®æ ¼å¼...")
    final_df = final_df.sort_values(by=line_id_col)
    final_df = final_df.rename(columns={line_id_col: 'OriginalLineId'})
    final_df.insert(0, 'NewLineId', range(1, len(final_df)+1))
    
    # ä¿å­˜ç»“æœ
    logging.info("ğŸ’¾ ä¿å­˜é‡‡æ ·ç»“æœ...")
    final_df.to_csv(output_file, index=False)
    logging.info(f"âœ… é‡‡æ ·å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ï¼š{output_file}")
    
    # æœ€ç»ˆç»Ÿè®¡
    logging.info("\nğŸ“Š æœ€ç»ˆæ ·æœ¬ç»Ÿè®¡ï¼š")
    logging.info(f"  - å¼‚å¸¸æ ·æœ¬ï¼š{final_anomaly_count} æ¡")
    logging.info(f"  - æ­£å¸¸æ ·æœ¬ï¼š{len(normal_sample)} æ¡")
    logging.info(f"  - æ€»è®¡ï¼š{len(final_df)} æ¡")

if __name__ == "__main__":
    # ç¤ºä¾‹è°ƒç”¨
    optimized_sampling(
        log_type="HDFS",
        radio=10,
        scheme="anomaly_labels" #normal_based | anomaly_labels
    )