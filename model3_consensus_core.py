import time
from model3_agent1 import model3_agent_a_infer
from model3_agent2 import model3_agent_b_infer
from model3_agent3 import model3_agent_c_infer
from model3_similarity_utils import compute_similarity_matrix
from model3_feedback_utils import build_next_prompts
from model3_vote_utils import weighted_vote

MAX_ROUNDS = 3

def consensus_inference(row):
    """
    å¯¹å•æ¡æ—¥å¿—æ‰§è¡Œå¤šè½®ååŒæ¨ç†ã€‚
    è¿”å›: (final_label, consensus_flag, metadata)
    """
    history = []
    prompts = ["", "", ""]  # åˆå§‹æç¤ºä¸ºç©º

    for round_id in range(1, MAX_ROUNDS + 1):
        print(f"\nğŸŒ€ ç¬¬ {round_id} è½®ååŒæ¨ç†...")

        # === æ¨ç†è°ƒç”¨ ===
        agents = [model3_agent_a_infer, model3_agent_b_infer, model3_agent_c_infer]
        results = []

        for i, agent in enumerate(agents):
            try:
                result = agent(row, prompt_override=prompts[i]) if round_id > 1 else agent(row)
                if result is None:
                    raise ValueError("ç©ºè¿”å›")
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {chr(65+i)} æ¨ç†å¤±è´¥: {e}")
                result = {"label": -1, "reason": "è°ƒç”¨å¤±è´¥", "score": 0.0}
            results.append(result)

        labels = [r["label"] for r in results]
        reasons = {k: r["reason"] for k, r in zip(["A", "B", "C"], results)}
        scores = [r["score"] for r in results]

        print("ğŸ§¾ å½“å‰æ¨ç†æ ‡ç­¾ï¼š", labels)
        print("ğŸ—£ï¸ å½“å‰è§£é‡Šæ‘˜è¦ï¼š", list(reasons.values()))

        # === å…±è¯†åˆ¤æ–­ ===
        if all(l == labels[0] and l in [0, 1] for l in labels):
            print("âœ… æ ‡ç­¾å®Œå…¨ä¸€è‡´ï¼Œç›´æ¥è¾“å‡º")
            return labels[0], "HARD", {
                "round": round_id,
                "reasons": reasons,
                "scores": scores,
                "method": "åˆè½®ä¸€è‡´"
            }

        # === è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ ===
        sim_avg, sim_matrix = compute_similarity_matrix(reasons)
        print(f"ğŸ”— å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦ Sim_avg = {sim_avg:.3f}")

        valid_labels = all(l in [0, 1] for l in labels)
        if sim_avg >= 0.85 and valid_labels:
            label_final, vote_map = weighted_vote(results, sim_matrix)
            print("ğŸ¤ é«˜åº¦ç›¸ä¼¼ï¼Œå…±è¯†æŠ•ç¥¨è¾“å‡º")
            return label_final, "WEAK", {
                "round": round_id,
                "reasons": reasons,
                "scores": scores,
                "vote_map": vote_map,
                "method": "åŠ æƒæŠ•ç¥¨"
            }

        # === å‡†å¤‡ä¸‹ä¸€è½® ===
        if round_id < MAX_ROUNDS:
            prompts, strategy_flag = build_next_prompts(row, results, sim_matrix, sim_avg)
            print(f"ğŸ› ï¸ ä½¿ç”¨æç¤ºç­–ç•¥ï¼š{strategy_flag}ï¼Œå‡†å¤‡è¿›å…¥ä¸‹ä¸€è½®")
            time.sleep(1)

        history.append({"labels": labels, "reasons": reasons, "scores": scores})

    # === è¾¾åˆ°æœ€å¤§è½®æ¬¡ä»æœªå…±è¯† ===
    print("âš ï¸ è¾¾åˆ°æœ€å¤§è½®æ•°ä»æœªæ”¶æ•›")
    return None, "FAIL", {"round": MAX_ROUNDS, "history": history}


# âœ… å•å…ƒæµ‹è¯•å…¥å£
if __name__ == "__main__":
    import pandas as pd

    test_path = "è§£æåçš„æ•°æ®é›†.csv"
    df = pd.read_csv(test_path)
    row = df.iloc[10]

    label, status, detail = consensus_inference(row)
    print("\nğŸ§¾ æœ€ç»ˆå…±è¯†æ ‡ç­¾:", label)
    print("ğŸ“Œ å…±è¯†çŠ¶æ€:", status)
    print("ğŸ“Š æ˜ç»†ä¿¡æ¯:", detail)
